---

![Python](https://img.shields.io/badge/Python-3.9-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-Deployed-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

# ğŸ§  Toxic Comment Classification

A multi-label text classification system to detect toxic, obscene, insulting, and hateful comments using:

- âœ… TF-IDF with ML models (Logistic Regression, SVM, Naive Bayes)
- ğŸ§  Deep learning architectures (BiLSTM, BiGRU, TextCNN)
- ğŸ¤– Transformers (DistilBERT)

The project compares performance across models, addresses class imbalance, and deploys results in a Streamlit app.

---

## ğŸ“š Table of Contents

- [Problem Statement](#problem-statement)
- [Business Use Case](#business-use-case)
- [Overview](#overview)
- [Objectives](#objectives)
- [Dataset](#dataset)
- [Data Preprocessing](#data-preprocessing)
- [Models](#models)
- [Results](#results)
- [Installation & Usage](#installation--usage)
- [Key Learnings](#key-learnings)
- [Project Structure](#project-structure)
- [Connect With Me](#connect-with-me)
- [Acknowledgements](#acknowledgements)

---

## ğŸ” Problem Statement

Online platforms struggle to moderate toxic content. This project builds a multi-label classification system to detect toxic, obscene, threatening, and hateful comments using classical ML, deep learning, and transformer-based models.

---

## ğŸ’¼ Business Use Case

Helps platforms like forums, social media, and comment sections automatically flag harmful content, improving user safety and experience.

---

## ğŸ“– Overview

This project focuses on detecting toxic language in online comments. The dataset contains user comments labeled into multiple categories of toxicity:

- toxic
- severe toxic
- obscene
- threat
- insult
- identity hate

It is a multi-label text classification problem â€” a comment may belong to more than one category.

---

## ğŸ–¼ï¸ Demo Preview

![App Screenshot](https://your-image-hosting.com/app-preview.png)

---

## ğŸ¯ Objectives

- Preprocess text (cleaning, tokenization, stopword removal)
- Apply TF-IDF vectorization for baseline models
- Build classical ML baselines (Logistic Regression, SVM, Naive Bayes)
- Explore deep learning models (BiLSTM, BiGRU, CNN)
- Fine-tune a Transformer (DistilBERT) for contextual understanding
- Evaluate and compare model performance

---

## ğŸ“Š Dataset

- Source: [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
- Train size: ~160k comments
- Test size: ~64k comments
- Labels: Multi-label, highly imbalanced

---

## ğŸ§¹ Data Preprocessing

1. Text Cleaning:
   - Lowercasing
   - Removing URLs, HTML tags, numbers, punctuation, and extra spaces
2. Tokenization & Stopword Removal (using NLTK)
3. Vectorization:
   - TF-IDF baseline (top 5000 features)
   - Tokenizer + Padding for deep models

---

## ğŸ§ª Models

### ğŸ”¹ Baseline (TF-IDF + Classical ML)

- Logistic Regression (One-vs-Rest)
- Linear SVM (One-vs-Rest)
- Multinomial Naive Bayes

### ğŸ”¹ Deep Learning Models

- BiLSTM (captures long-range sequential dependencies)
- BiGRU (lightweight alternative to LSTM)
- TextCNN (captures local n-gram patterns)

### ğŸ”¹ Transformers

- DistilBERT Fine-Tuning (contextual embeddings, best for nuanced toxic patterns)

---

## ğŸ“ˆ Results

### âœ… Baseline Models

| Model                   | Accuracy | Macro Precision | Macro Recall | Macro F1 |
|------------------------|----------|-----------------|--------------|----------|
| Logistic Regression     | 91.9%    | 0.76            | 0.38         | 0.49     |
| Linear SVM              | 93.1%    | 0.86            | 0.55         | 0.66     |
| Multinomial Naive Bayes | 91.4%    | 0.63            | 0.31         | 0.41     |

### ğŸ” Deep Learning Highlights

- BiLSTM: Strong on frequent labels (toxic, obscene, insult), weaker on rare ones
- TextCNN: Improved recall for minority labels like `threat`
- BiGRU: Balanced performance with faster training

### ğŸ§  Transformer (DistilBERT)

- Outperformed classical and RNN/CNN models
- Best macro F1 across all categories

---

## âš™ï¸ Installation & Usage

### Environment Setup

```bash
git clone https://github.com/ut-si-ch/Toxic-Comment-Classification.git
cd Toxic-Comment-Classification

conda create -n toxicity_env python=3.9
conda activate toxicity_env

pip install -r requirements.txt
```

### Run Notebook

```bash
jupyter notebook Toxicity_Project.ipynb
```

### Streamlit App (Deployment)

```bash
streamlit run app.py
```

---

## ğŸ§  Key Learnings

- Classical ML (TF-IDF + SVM) provides strong baselines
- Deep models (BiLSTM, GRU, CNN) learn semantic and sequential patterns
- Transformers (DistilBERT) capture context & nuance best, especially for rare labels
- Class imbalance significantly impacts minority label detection (handled with sample weighting / focal loss)

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/                  # Dataset (not included, add Kaggle link)
â”œâ”€â”€ notebooks/             # Jupyter notebooks
â”œâ”€â”€ saved_models/          # Trained models
â”œâ”€â”€ app.py                 # Streamlit deployment script
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ Toxicity_Project.ipynb # Main notebook
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ“¬ Connect With Me

- [LinkedIn](https://www.linkedin.com/in/uttam-singh-chaudhary-98408214b)
- [Portfolio](https://datascienceportfol.io/uttamsinghchaudhary)
- [Email](mailto:uttamsinghchaudhary@gmail.com)

---

## ğŸ™ Acknowledgements

- [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
- Hugging Face Transformers
- TensorFlow & Scikit-learn

---
